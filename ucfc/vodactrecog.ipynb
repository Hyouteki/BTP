{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://youtu.be/QmtSkq3DYko?si=6VzZc_NH5glCPi0m\n",
    "- https://learnopencv.com/introduction-to-video-classification-and-human-activity-recognition/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed as randomSeed, choice\n",
    "from numpy import asarray, array\n",
    "from numpy.random import seed as numpyRandomSeed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import (\n",
    "    ConvLSTM2D,\n",
    "    MaxPooling3D,\n",
    "    TimeDistributed,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Dense\n",
    ")\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.random import set_seed as tensorflowRandomSeed\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from cv2 import (\n",
    "    VideoCapture, \n",
    "    cvtColor, \n",
    "    COLOR_BGR2RGB,\n",
    "    putText,\n",
    "    FONT_HERSHEY_SIMPLEX,\n",
    "    CAP_PROP_FRAME_COUNT,\n",
    "    CAP_PROP_POS_FRAMES,\n",
    "    resize,\n",
    ")\n",
    "from matplotlib.pyplot import (\n",
    "    figure, \n",
    "    subplot, \n",
    "    plot, \n",
    "    imshow, \n",
    "    axis, \n",
    "    title, \n",
    "    legend\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardcoding SEED to make results consistent with every execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 27\n",
    "numpyRandomSeed(SEED)\n",
    "randomSeed(SEED)\n",
    "tensorflowRandomSeed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading and extracting UCF50 dataset<br>\n",
    "source: https://www.crcv.ucf.edu/data/UCF50.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# !wget --no-check-certificate https://www.crcv.ucf.edu/data/UCF50.rar\n",
    "\n",
    "# uncomment this to unrar the rar dataset file \\\n",
    "# or use some unpacking software like 7-ZIP like I did\n",
    "# !unrar x UCF50.rar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the UTF50 dataset, not necessary to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20, 20))\n",
    "allClassNames = listdir(\"UCF50\")\n",
    "print(allClassNames)\n",
    "samplesInEachClass = [len(listdir(join(\"UCF50\", i))) for i in allClassNames]\n",
    "print(samplesInEachClass)\n",
    "\n",
    "for i in range(len(allClassNames)):\n",
    "    # getting paths of all the videos in class[i]\n",
    "    allVideosInClass = listdir(join(\"UCF50\", allClassNames[i]))\n",
    "    # pick a random video to show onto the subplot\n",
    "    randomSelectedVideo = choice(allVideosInClass)\n",
    "    videoReader = VideoCapture(join(\"UCF50\", allClassNames[i], randomSelectedVideo))\n",
    "    success, bgrFrame = videoReader.read()\n",
    "    # if not successful in reading a frame break from the loop\n",
    "    if not success:\n",
    "        break\n",
    "    videoReader.release()\n",
    "    # converting frame from BGR to RGB\n",
    "    rgbFrame = cvtColor(bgrFrame, COLOR_BGR2RGB)\n",
    "    # writing class label on the sample image\n",
    "    putText(\n",
    "        rgbFrame,\n",
    "        allClassNames[i],\n",
    "        (10, 30),\n",
    "        FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (255, 255, 255),\n",
    "        2,\n",
    "    )\n",
    "    # putting the frame with class label onto the subplot\n",
    "    subplot(5, 4, i + 1)\n",
    "    imshow(rgbFrame)\n",
    "    axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame dimensions\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_DIMENSION = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "\n",
    "# number of frames present in one feature\n",
    "SEQUENCE_LENGTH = 20\n",
    "\n",
    "# dataset name\n",
    "DATASET_DIR = \"UCF50\"\n",
    "\n",
    "# classes to train upon\n",
    "CLASSES = [\"BenchPress\", \"CleanAndJerk\", \"Diving\", \"BreastStroke\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frameExtraction(videoPath):\n",
    "    \"\"\"\n",
    "    @desc: extract frames from a video at videoPath\n",
    "    @param {string} videoPath: path of a video\n",
    "    @returns {list} frames: `SEQUENCE_LENGTH` number of frames that are \\\n",
    "        equally spaced out in the video \n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    videoReader = VideoCapture(videoPath)\n",
    "    # total number of frames present in the video\n",
    "    frameCount = int(videoReader.get(CAP_PROP_FRAME_COUNT))\n",
    "    skipFrameWindow = max(int(frameCount / SEQUENCE_LENGTH), 1)\n",
    "    for i in range(SEQUENCE_LENGTH):\n",
    "        videoReader.set(CAP_PROP_POS_FRAMES, i * skipFrameWindow)\n",
    "        success, frame = videoReader.read()\n",
    "        # if not successful in reading the frame break from the loop\n",
    "        if not success:\n",
    "            break\n",
    "        # append the frame on frames after resizing\n",
    "        frames.append(resize(frame, IMAGE_DIMENSION) / 255)\n",
    "    videoReader.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features and labels from `CLASSES` (train classses)\n",
    "- *{2D vector} features*: vector of feature (vector of frame in a video)\n",
    "- *{2D vector} oneHotEncodedLabels*: vector of hotEncodedLabel corresponding to a feature\n",
    "  - Ex. [1 0 0 0]: meaning that the corresponding feature belongs to class[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = [], []\n",
    "for classId, className in enumerate(CLASSES):\n",
    "    print(f\"Extracting Data of Class: {className}\")\n",
    "    files = listdir(join(DATASET_DIR, className))\n",
    "    for file in files:\n",
    "        videoFilePath = join(DATASET_DIR, className, file)\n",
    "        frames = frameExtraction(videoFilePath)\n",
    "        if len(frames) == SEQUENCE_LENGTH:\n",
    "            features.append(frames)\n",
    "            labels.append(classId)\n",
    "features = asarray(features)\n",
    "labels = array(labels)\n",
    "oneHotEncodedLabels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the features and labels into train and test dataset with test_size = 0.2 and shuffling enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresTrain, featuresTest, labelsTrain, labelsTest = train_test_split(\n",
    "    features, oneHotEncodedLabels, test_size=0.2, shuffle=True, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: \"sequential\"\n",
    "\n",
    "| Layer (type) | Output Shape | Param |   \n",
    "| :----------- | :----------- | :---- |\n",
    "| conv_lstm2d (ConvLSTM2D) | (None, 20, 62, 62, 4) | 1024 |            \n",
    "| max_pooling3d (MaxPooling3D) | (None, 20, 31, 31, 4) | 0 |\n",
    "| time_distributed (TimeDistributed) | (None, 20, 31, 31, 4) | 0 | \n",
    "| conv_lstm2d_1 (ConvLSTM2D) | (None, 20, 29, 29, 8) | 3488 |      \n",
    "| max_pooling3d_1 (MaxPooling3D) | (None, 20, 15, 15, 8) | 0 |                                                                    \n",
    "| time_distributed_1 (TimeDistributed) | (None, 20, 15, 15, 8) | 0 |         \n",
    "| conv_lstm2d_2 (ConvLSTM2D) | (None, 20, 13, 13, 14) | 11144 |    \n",
    "| max_pooling3d_2 (MaxPooling3D) | (None, 20, 7, 7, 14) | 0 |                                                        \n",
    "| time_distributed_2 (TimeDistributed) | (None, 20, 7, 7, 14) | 0 |                                                      \n",
    "| conv_lstm2d_3 (ConvLSTM2D) | (None, 20, 5, 5, 16) | 17344 |    \n",
    "| max_pooling3d_3 (MaxPooling3D) | (None, 20, 3, 3, 16) | 0 |                                                            \n",
    "| flatten (Flatten) | (None, 2880) | 0 |        \n",
    "| dense (Dense) | (None, 4) | 11524 |    \n",
    "                                                                 \n",
    "- Total params: 44524 (173.92 KB)\n",
    "- Trainable params: 44524 (173.92 KB)\n",
    "- Non-trainable params: 0 (0.00 Byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModelArchitecture():\n",
    "    model = Sequential(\n",
    "        [\n",
    "            ConvLSTM2D(\n",
    "                filters=4,\n",
    "                kernel_size=(3, 3),\n",
    "                activation=\"tanh\",\n",
    "                data_format=\"channels_last\",\n",
    "                recurrent_dropout=0.2,\n",
    "                return_sequences=True,\n",
    "                input_shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3),\n",
    "            ),\n",
    "            MaxPooling3D(\n",
    "                pool_size=(1, 2, 2), padding=\"same\", data_format=\"channels_last\"\n",
    "            ),\n",
    "            TimeDistributed(Dropout(0.2)),\n",
    "            ConvLSTM2D(\n",
    "                filters=8,\n",
    "                kernel_size=(3, 3),\n",
    "                activation=\"tanh\",\n",
    "                data_format=\"channels_last\",\n",
    "                recurrent_dropout=0.2,\n",
    "                return_sequences=True,\n",
    "            ),\n",
    "            MaxPooling3D(\n",
    "                pool_size=(1, 2, 2), padding=\"same\", data_format=\"channels_last\"\n",
    "            ),\n",
    "            TimeDistributed(Dropout(0.2)),\n",
    "            ConvLSTM2D(\n",
    "                filters=14,\n",
    "                kernel_size=(3, 3),\n",
    "                activation=\"tanh\",\n",
    "                data_format=\"channels_last\",\n",
    "                recurrent_dropout=0.2,\n",
    "                return_sequences=True,\n",
    "            ),\n",
    "            MaxPooling3D(\n",
    "                pool_size=(1, 2, 2), padding=\"same\", data_format=\"channels_last\"\n",
    "            ),\n",
    "            TimeDistributed(Dropout(0.2)),\n",
    "            ConvLSTM2D(\n",
    "                filters=16,\n",
    "                kernel_size=(3, 3),\n",
    "                activation=\"tanh\",\n",
    "                data_format=\"channels_last\",\n",
    "                recurrent_dropout=0.2,\n",
    "                return_sequences=True,\n",
    "            ),\n",
    "            MaxPooling3D(\n",
    "                pool_size=(1, 2, 2), padding=\"same\", data_format=\"channels_last\"\n",
    "            ),\n",
    "            Flatten(),\n",
    "            Dense(len(CLASSES), activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model object\n",
    "model = createModelArchitecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compiling and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for info on early stopping callback refer the references mentioned at top\n",
    "earlyStoppingCallback = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=10, mode=\"min\", restore_best_weights=True\n",
    ")\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "modelTrainingHistory = model.fit(\n",
    "    x=featuresTrain,\n",
    "    y=labelsTrain,\n",
    "    epochs=7,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[earlyStoppingCallback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(modelTrainingHistory, metricName1, metricName2, plotName):\n",
    "    \"\"\"\n",
    "    @desc: plots the training history of `metricName1` & `metricName2` \\\n",
    "        using `modelTrainingHistory`\n",
    "    \"\"\"\n",
    "    metricValue1 = modelTrainingHistory.history[metricName1]\n",
    "    metricValue2 = modelTrainingHistory.history[metricName2]\n",
    "    epochs = range(len(metricValue1))\n",
    "    plot(epochs, metricValue1, \"blue\", label=metricName1)\n",
    "    plot(epochs, metricValue2, \"red\", label=metricName2)\n",
    "    title(str(plotName))\n",
    "    legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(\n",
    "    modelTrainingHistory, \"loss\", \"val_loss\", \"Total Loss vs Total Validation Loss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![training_history](ucf_training_history.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"{DATASET_DIR}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(f\"{DATASET_DIR}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(featuresTest, labelsTest)\n",
    "print(loss)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictVideo(videoPath):\n",
    "    \"\"\"\n",
    "    @desc: predicts the video based on the model trained above\n",
    "    @param {string} videoPath: the path to the video\n",
    "    @returns {string} className: class that the model thinks given video belongs to \n",
    "    \"\"\"\n",
    "    frames = frameExtraction(videoPath)\n",
    "    confidences = model.predict(asarray([frames]))\n",
    "    i = 0\n",
    "    confidence = confidences[i]\n",
    "    for j in range(1, len(confidences)):\n",
    "        if j > confidence:\n",
    "            confidence = confidences[j]\n",
    "            i = j\n",
    "    return CLASSES[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictVideo(r\"UCF50/BenchPress/v_BenchPress_g01_c01.avi\")\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
